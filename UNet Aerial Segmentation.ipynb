{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Github U-Net model for the automatic Segmentation of aerial images using Dubai’s satellite imagery dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Create a Google Drive folder path"
      ],
      "metadata": {
        "id": "ve1aRxywzAGf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOhLXNMQ0v73"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "YCeb-xWrqp6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "Qki-WoWT1gX-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9E_ChUd1JVd"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "from random import shuffle\n",
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "from glob import glob\n",
        "import sys\n",
        "import shutil  \n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a folder in MyDrive\n",
        "Create a folder in MyDrive named \"UNet-AerialSegmentation\"\n",
        "and Change directory pointing this folder"
      ],
      "metadata": {
        "id": "_Ssc1nnZNkMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/UNet-AerialSegmentation"
      ],
      "metadata": {
        "id": "GfqiWzoGq3Ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WNaKKfp1PYq"
      },
      "source": [
        "# Download data\n",
        "Download \"Semantic segmentation of aerial imagery” Dataset and upload the dataset in the \"UNet-AerialSegmentation\" folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDRdk-Xq1WpV"
      },
      "source": [
        "# Data loader and data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6nhF1um1VBZ"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import PIL\n",
        "import random\n",
        "from scipy import ndimage\n",
        "\n",
        "\n",
        "class segDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, training, transform=None):\n",
        "        super(segDataset, self).__init__()\n",
        "        self.root = root\n",
        "        self.training = training\n",
        "        self.transform = transform\n",
        "        self.IMG_NAMES = sorted(glob(self.root + '/*/images/*.jpg'))\n",
        "        self.BGR_classes = {'Water' : [ 41, 169, 226],\n",
        "                            'Land' : [246,  41, 132],\n",
        "                            'Road' : [228, 193, 110],\n",
        "                            'Building' : [152,  16,  60], \n",
        "                            'Vegetation' : [ 58, 221, 254],\n",
        "                            'Unlabeled' : [155, 155, 155]} # in BGR\n",
        "\n",
        "        self.bin_classes = ['Water', 'Land', 'Road', 'Building', 'Vegetation', 'Unlabeled']\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.IMG_NAMES[idx]\n",
        "        mask_path = img_path.replace('images', 'masks').replace('.jpg', '.png')\n",
        "\n",
        "        image = cv2.imread(img_path)\n",
        "        mask = cv2.imread(mask_path)\n",
        "        cls_mask = np.zeros(mask.shape)  \n",
        "        cls_mask[mask == self.BGR_classes['Water']] = self.bin_classes.index('Water')\n",
        "        cls_mask[mask == self.BGR_classes['Land']] = self.bin_classes.index('Land')\n",
        "        cls_mask[mask == self.BGR_classes['Road']] = self.bin_classes.index('Road')\n",
        "        cls_mask[mask == self.BGR_classes['Building']] = self.bin_classes.index('Building')\n",
        "        cls_mask[mask == self.BGR_classes['Vegetation']] = self.bin_classes.index('Vegetation')\n",
        "        cls_mask[mask == self.BGR_classes['Unlabeled']] = self.bin_classes.index('Unlabeled')\n",
        "        cls_mask = cls_mask[:,:,0] \n",
        "\n",
        "        if self.training==True:\n",
        "            if self.transform:\n",
        "              image = transforms.functional.to_pil_image(image)\n",
        "              image = self.transform(image)\n",
        "              image = np.array(image)\n",
        "\n",
        "            # 90 degree rotation\n",
        "            if np.random.rand()<0.5:\n",
        "              angle = np.random.randint(4) * 90\n",
        "              image = ndimage.rotate(image,angle,reshape=True)\n",
        "              cls_mask = ndimage.rotate(cls_mask,angle,reshape=True)\n",
        "\n",
        "            # vertical flip\n",
        "            if np.random.rand()<0.5:\n",
        "              image = np.flip(image, 0)\n",
        "              cls_mask = np.flip(cls_mask, 0)\n",
        "            \n",
        "            # horizonal flip\n",
        "            if np.random.rand()<0.5:\n",
        "              image = np.flip(image, 1)\n",
        "              cls_mask = np.flip(cls_mask, 1)\n",
        "\n",
        "        image = cv2.resize(image, (512,512))/255.0\n",
        "        cls_mask = cv2.resize(cls_mask, (512,512)) \n",
        "        image = np.moveaxis(image, -1, 0)\n",
        "\n",
        "        return torch.tensor(image).float(), torch.tensor(cls_mask, dtype=torch.int64)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.IMG_NAMES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCNI4Ap01alL",
        "outputId": "e173553e-27a5-460e-ca5f-14034467a43c"
      },
      "source": [
        "color_shift = transforms.ColorJitter(.1,.1,.1,.1)\n",
        "blurriness = transforms.GaussianBlur(3, sigma=(0.1, 2.0))\n",
        "\n",
        "t = transforms.Compose([color_shift, blurriness])\n",
        "dataset = segDataset('./Semantic segmentation dataset', training = True, transform= t)\n",
        "\n",
        "len(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "72"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HO-ErIX71b-j"
      },
      "source": [
        "d = dataset[1]\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(np.moveaxis(d[0].numpy(),0,-1))\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(d[1].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlpEdstKPyqg",
        "outputId": "d992c656-662b-4644-9e2f-9452b52537c9"
      },
      "source": [
        "test_num = int(0.1 * len(dataset))\n",
        "print(f'test data : {test_num}')\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset)-test_num, test_num], generator=torch.Generator().manual_seed(101))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test data : 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1_T8eQ51iRn"
      },
      "source": [
        "BACH_SIZE = 4\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=BACH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=BACH_SIZE, shuffle=False, num_workers=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y70ECc-y1i4K"
      },
      "source": [
        "# U-net model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaugx4aaO9sR"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "        # if you have padding issues, see\n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APxNt4tuQako"
      },
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = Down(512, 1024 // factor)\n",
        "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
        "        self.up2 = Up(512, 256 // factor, bilinear)\n",
        "        self.up3 = Up(256, 128 // factor, bilinear)\n",
        "        self.up4 = Up(128, 64, bilinear)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Focal loss"
      ],
      "metadata": {
        "id": "g2TPFlJubW97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
        "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
        "        self.size_average = size_average\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        if input.dim()>2:\n",
        "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
        "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
        "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
        "        target = target.view(-1,1)\n",
        "\n",
        "        logpt = F.log_softmax(input, dim=-1)\n",
        "        logpt = logpt.gather(1,target)\n",
        "        logpt = logpt.view(-1)\n",
        "        pt = Variable(logpt.data.exp())\n",
        "\n",
        "        if self.alpha is not None:\n",
        "            if self.alpha.type()!=input.data.type():\n",
        "                self.alpha = self.alpha.type_as(input.data)\n",
        "            at = self.alpha.gather(0,target.data.view(-1))\n",
        "            logpt = logpt * Variable(at)\n",
        "\n",
        "        loss = -1 * (1-pt)**self.gamma * logpt\n",
        "        if self.size_average: return loss.mean()\n",
        "        else: return loss.sum()"
      ],
      "metadata": {
        "id": "cMUombXsbVk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = FocalLoss(gamma=3/4).to(device)"
      ],
      "metadata": {
        "id": "EKNmaUnXbbdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEbHoCu8SP0_"
      },
      "source": [
        "# IoU-Based loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vywh2VYYSQpM"
      },
      "source": [
        "class mIoULoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True, n_classes=2):\n",
        "        super(mIoULoss, self).__init__()\n",
        "        self.classes = n_classes\n",
        "\n",
        "    def to_one_hot(self, tensor):\n",
        "        n,h,w = tensor.size()\n",
        "        one_hot = torch.zeros(n,self.classes,h,w).to(tensor.device).scatter_(1,tensor.view(n,1,h,w),1)\n",
        "        return one_hot\n",
        "\n",
        "    def forward(self, inputs, target):\n",
        "        # inputs => N x Classes x H x W\n",
        "        # target_oneHot => N x Classes x H x W\n",
        "\n",
        "        N = inputs.size()[0]\n",
        "\n",
        "        # predicted probabilities for each pixel along channel\n",
        "        inputs = F.softmax(inputs,dim=1)\n",
        "        \n",
        "        # Numerator Product\n",
        "        target_oneHot = self.to_one_hot(target)\n",
        "        inter = inputs * target_oneHot\n",
        "        ## Sum over all pixels N x C x H x W => N x C\n",
        "        inter = inter.view(N,self.classes,-1).sum(2)\n",
        "\n",
        "        #Denominator \n",
        "        union= inputs + target_oneHot - (inputs*target_oneHot)\n",
        "        ## Sum over all pixels N x C x H x W => N x C\n",
        "        union = union.view(N,self.classes,-1).sum(2)\n",
        "\n",
        "        loss = inter/union\n",
        "\n",
        "        ## Return average loss over classes and batch\n",
        "        return 1-loss.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FTvBfpMSTR9"
      },
      "source": [
        "criterion = mIoULoss(n_classes=6).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9V6KTXU4wdR"
      },
      "source": [
        "# Training task and Accuracy metrics computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZqa0-aofjge"
      },
      "source": [
        "def acc(label, predicted):\n",
        "  seg_acc = (y.cpu() == torch.argmax(pred_mask, axis=1).cpu()).sum() / torch.numel(y.cpu())\n",
        "  return seg_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = (y_true.cpu() == torch.argmax(y_pred, axis=1).cpu()).sum()\n",
        "    possible_positives = torch.sum(torch.clip(y_true,0,1))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = (y_true.cpu() == torch.argmax(y_pred, axis=1).cpu()).sum()\n",
        "    possible_positives = torch.sum(torch.clip(y_pred,0,1))\n",
        "    precision = true_positives / (possible_positives + K.epsilon())\n",
        "\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kcY4-JyJRqjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2URZk5o240xZ"
      },
      "source": [
        "min_loss = torch.tensor(float('inf'))\n",
        "\n",
        "model = UNet(n_channels=3, n_classes=6, bilinear=True).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SUA50ca49LE"
      },
      "source": [
        "os.makedirs('./saved_models', exist_ok=True)\n",
        "\n",
        "N_EPOCHS = 100\n",
        "N_DATA = len(train_dataset)\n",
        "N_TEST = len(test_dataset)\n",
        "\n",
        "plot_losses = []\n",
        "plot_prec=[]\n",
        "plot_recall=[]\n",
        "plot_f1_score=[]\n",
        "scheduler_counter = 0\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "  # training\n",
        "  model.train()\n",
        "  loss_list = []\n",
        "  acc_list = []\n",
        "  prec_list=[]\n",
        "  recall_list=[]\n",
        "  f1_list=[]\n",
        "\n",
        "  for batch_i, (x, y) in enumerate(train_dataloader):\n",
        "\n",
        "      pred_mask = model(x.to(device))  \n",
        "      loss = criterion(pred_mask, y.to(device))\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      loss_list.append(loss.cpu().detach().numpy())\n",
        "      acc_list.append(acc(y,pred_mask).numpy())\n",
        "      recall_list.append(recall_m(y,pred_mask).cpu().detach().numpy())\n",
        "      prec_list.append(precision_m(y,pred_mask).cpu().detach().numpy())\n",
        "      f1_list.append(f1_m(y,pred_mask).cpu().detach().numpy())\n",
        "      sys.stdout.write(\n",
        "          \"\\r[Epoch %d/%d] [Batch %d/%d] [Loss: %f (%f)]\"\n",
        "          % (\n",
        "              epoch,\n",
        "              N_EPOCHS,\n",
        "              batch_i,\n",
        "              len(train_dataloader),\n",
        "              loss.cpu().detach().numpy(),\n",
        "              np.mean(loss_list),\n",
        "          )\n",
        "      )\n",
        "  scheduler_counter += 1\n",
        "  # testing\n",
        "  model.eval()\n",
        "  val_loss_list = []\n",
        "  val_acc_list = []\n",
        "  val_prec_list=[]\n",
        "  val_recall_list=[]\n",
        "  val_f1_list=[]\n",
        "  for batch_i, (x, y) in enumerate(test_dataloader):\n",
        "      with torch.no_grad():    \n",
        "          pred_mask = model(x.to(device))  \n",
        "      val_loss = criterion(pred_mask, y.to(device))\n",
        "      val_loss_list.append(val_loss.cpu().detach().numpy())\n",
        "      val_acc_list.append(acc(y,pred_mask).numpy())\n",
        "      val_prec_list.append(precision_m(y,pred_mask).cpu().detach().numpy())\n",
        "      val_recall_list.append(recall_m(y,pred_mask).cpu().detach().numpy())\n",
        "      val_f1_list.append(f1_m(y,pred_mask).detach().cpu().numpy())\n",
        "     \n",
        " \n",
        "  print(' epoch {} - loss : {:.5f} - acc : {:.2f} - val loss : {:.5f} - val acc : {:.2f} - prec : {:.2f} - recall : {:.2f} - f1_score : {:.2f} - val_prec : {:.2f} - val_recall : {:.2f} - val_f1_score : {:.2f}'.format(epoch, \n",
        "                                                                                                                                                                                                                         np.mean(loss_list), \n",
        "                                                                                                                                                                                                                         np.mean(acc_list),\n",
        "                                                                                                                                                                                                                         np.mean(val_loss_list),\n",
        "                                                                                                                                                                                                                         np.mean(val_acc_list),\n",
        "                                                                                                                                                                                                                         np.mean(prec_list),\n",
        "                                                                                                                                                                                                                         np.mean(recall_list),\n",
        "                                                                                                                                                                                                                         np.mean(f1_list),\n",
        "                                                                                                                                                                                                                         np.mean(val_prec_list),\n",
        "                                                                                                                                                                                                                         np.mean(val_recall_list),\n",
        "                                                                                                                                                                                                                         np.mean(val_f1_list)))\n",
        "  plot_losses.append([epoch, np.mean(loss_list), np.mean(val_loss_list)])\n",
        "  plot_prec.append([epoch, np.mean(prec_list),np.mean(val_prec_list)])\n",
        "  plot_recall.append([epoch, np.mean(recall_list),np.mean(val_recall_list)])\n",
        "  plot_f1_score.append([epoch, np.mean(f1_list),np.mean(val_f1_list)])\n",
        "\n",
        "  compare_loss = np.mean(val_loss_list)\n",
        "  is_best = compare_loss < min_loss\n",
        "  if is_best == True:\n",
        "    scheduler_counter = 0\n",
        "    min_loss = min(compare_loss, min_loss)\n",
        "    torch.save(model.state_dict(), './saved_models/unet_epoch_{}_{:.5f}.pt'.format(epoch,np.mean(val_loss_list)))\n",
        "  \n",
        "  if scheduler_counter > 5:\n",
        "    lr_scheduler.step()\n",
        "    print(f\"lowering learning rate to {optimizer.param_groups[0]['lr']}\")\n",
        "    scheduler_counter = 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Graph Plots"
      ],
      "metadata": {
        "id": "hsZzKdX1b7CM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eGs08A69vPO"
      },
      "source": [
        "# plot loss\n",
        "plot_losses = np.array(plot_losses)\n",
        "plt.plot(plot_losses[:,0], plot_losses[:,1], color='b', linewidth=2)\n",
        "plt.plot(plot_losses[:,0], plot_losses[:,2], color='r', linewidth=2)\n",
        "plt.title('mIoU Loss', fontsize=20)\n",
        "plt.xlabel('Epoch',fontsize=20)\n",
        "plt.ylabel('loss',fontsize=20)\n",
        "plt.grid()\n",
        "plt.legend(['Training', 'Validation']) # using a named size\n",
        "plt.show()\n",
        "#plot Precision\n",
        "plot_prec = np.array(plot_prec)\n",
        "plt.plot(plot_prec[:,0], plot_prec[:,1], color='b', linewidth=2)\n",
        "plt.plot(plot_prec[:,0], plot_prec[:,2], color='r', linewidth=2)\n",
        "plt.title('Precision', fontsize=20)\n",
        "plt.xlabel('Epoch',fontsize=20)\n",
        "plt.ylabel('Precision',fontsize=20)\n",
        "plt.grid()\n",
        "plt.legend(['Training', 'Validation']) # using a named size\n",
        "plt.show()\n",
        "#plot Recall\n",
        "plot_recall = np.array(plot_recall)\n",
        "plt.plot(plot_recall[:,0], plot_recall[:,1], color='b', linewidth=2)\n",
        "plt.plot(plot_recall[:,0], plot_recall[:,2], color='r', linewidth=2)\n",
        "plt.title('Recall', fontsize=20)\n",
        "plt.xlabel('Epoch',fontsize=20)\n",
        "plt.ylabel('Recall',fontsize=20)\n",
        "plt.grid()\n",
        "plt.legend(['Training', 'Validation']) # using a named size\n",
        "plt.show()\n",
        "#plot F1-Score\n",
        "plot_f1_score = np.array(plot_f1_score)\n",
        "plt.plot(plot_f1_score[:,0], plot_f1_score[:,1], color='b', linewidth=2)\n",
        "plt.plot(plot_f1_score[:,0], plot_f1_score[:,2], color='r', linewidth=2)\n",
        "plt.title('F1-Score', fontsize=20)\n",
        "plt.xlabel('Epoch',fontsize=20)\n",
        "plt.ylabel('F1-Score',fontsize=20)\n",
        "plt.grid()\n",
        "plt.legend(['Training', 'Validation']) # using a named size\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z68ALktN6wB7"
      },
      "source": [
        "# Loading a pre-trained model and application of the model on an aerial image "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH31qRzp6xep",
        "outputId": "a47355d9-c078-4ab4-d1e2-aaa3b8c157f0"
      },
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/UNet-AerialSegmentation/saved_models/unet_epoch_95_0.62743.pt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9zquIrq7Afg"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "for batch_i, (x, y) in enumerate(test_dataloader):\n",
        "    for j in range(len(x)):\n",
        "        result = model(x.to(device)[j:j+1])\n",
        "        mask = torch.argmax(result, axis=1).cpu().detach().numpy()[0]\n",
        "        im = np.moveaxis(x.to(device)[j].cpu().detach().numpy(), 0, -1).copy()*255\n",
        "        im = im.astype(int)\n",
        "        gt_mask = y[j]\n",
        "\n",
        "        plt.figure(figsize=(12,12))\n",
        "\n",
        "        plt.subplot(1,3,1)\n",
        "        im = np.moveaxis(x.to(device)[j].cpu().detach().numpy(), 0, -1).copy()*255\n",
        "        im = im.astype(int)\n",
        "        plt.imshow(im)\n",
        "\n",
        "        plt.subplot(1,3,2)\n",
        "        plt.imshow(gt_mask)\n",
        "\n",
        "        plt.subplot(1,3,3)\n",
        "        plt.imshow(mask)\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/Colab Notebooks/ortofoto/Scavo75_clip_rect.tif'\n",
        "img = cv2.imread(path)\n",
        "img = cv2.resize(img, (512,512))/255.0\n",
        "test_img = np.moveaxis(img, -1, 0)\n",
        "test_img = torch.from_numpy(np.array([test_img]).astype(dtype='float32'))\n",
        "\n",
        "t_result = model(test_img.to(device))\n",
        "\n",
        "mask = torch.argmax(t_result, axis=1).cpu().detach().numpy()[0]\n",
        "\n",
        "im = np.moveaxis(x.to(device)[j].cpu().detach().numpy(), 0, -1).copy()*255\n",
        "im = im.astype(int)\n",
        "gt_mask = y[j]\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "im = np.moveaxis(x.to(device)[j].cpu().detach().numpy(), 0, -1).copy()*255\n",
        "im = im.astype(int)\n",
        "plt.imshow(img)\n",
        "\n",
        "# plt.subplot(1,3,2)\n",
        "# plt.imshow(gt_mask)\n",
        "sp = path.split('.')\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.imshow(mask)\n",
        "plt.savefig(sp[0]+'_mask.'+sp[1])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2e_8wW4bCv3L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}